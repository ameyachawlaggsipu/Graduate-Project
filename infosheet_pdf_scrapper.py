# -*- coding: utf-8 -*-
"""InfoSheet PDF Scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5NBJlMkP2cXF65JVcpLYAjgBy9_bBHV

**Generating Mapping between Parent Class, Child Class and the URL\**
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm


# URL of the website you want to scrape
url = 'https://www.ementalhealth.ca/Toronto/ArticlesByCategory/index.php?m=articlesByCategory'

# Send a GET request to the website
response = requests.get(url)

# Check if the request was successful (status code 200)

    # Get the HTML content of the website
html_content = response.text

    # Create a BeautifulSoup object
soup = BeautifulSoup(html_content, 'html.parser')

# Find all <td> tags
td_tags = soup.find_all('td')
pairs = []
# Iterate through each <td> tag
for td in td_tags:
    # Check if the <td> tag contains a <div> tag
    div_tags = td.find_all('div')

    # Iterate through each <div> tag
    for div in div_tags:
        # Print the content of the <td> tag with <div> tag
        if div.findAll('b'):
    #        print("Content of <td> with <div> tag:")
   #         print(td.findAll('b')[0].text.strip())
            name = td.findAll('b')[0].text.strip()
            continue
        # Find all <a> tags within the <div> tag
        a_tags = div.find_all('a')

        # Iterate through each <a> tag
        for a in a_tags:
                 # Print the text of the <a> tag
  #        print("Link:", a.text.strip())
            # Print the href attribute of the <a> tag
 #         print("Href:", a.get('href'))
          filename = a.text.strip()
          filename = ''.join([x for x in filename if x.isalnum()])
          pairs.append((filename,name,"https://www.ementalhealth.ca/" + a.get('href')))
#        print()  # Add a newline for better readability

data = pd.DataFrame(pairs,columns=['Child Class','Parent Class','URL'])

data.to_csv('Mapping.csv')

"""**Downloading the PDFS and saving in /docs/ **"""

for pair in pairs:
  name = pair[0]
  test = pair[2]
  response = requests.get(test)
  html_content = response.text

    # Create a BeautifulSoup object
  soup = BeautifulSoup(html_content, 'html.parser')
  pdf_a_tags = soup.find_all('a', text='PDF')
  target_url = 'https://www.ementalhealth.ca' + pdf_a_tags[0].get('href')
  response = requests.get(target_url)
  with open('/content/docs/'+ name + '.pdf', 'wb') as f:
        f.write(response.content)

"""**Zipping the docs folder**"""


!zip -r /content/dos.zip /content/docs

